{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PGM-Lab/2023-RateFunction/blob/main/Figure9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2EOnCShVSJ4"
      },
      "source": [
        "# Set-Up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXsO6OyvFiJz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
        "\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezfO-JPQ9JNs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "assert(tf.test.gpu_device_name())\n",
        "tf.keras.backend.clear_session()\n",
        "tf.config.optimizer.set_jit(True) # Enable XLA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyjb_iYxcdCR"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXIhRtCfxCFB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "train_size = 1000\n",
        "\n",
        "def load_data(data_set, train_size):\n",
        "  #(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "  (x_test, y_test), (x_train, y_train) = datasets.mnist.load_data()\n",
        "\n",
        "  tf.keras.utils.set_random_seed(123)\n",
        "  a = np.random.permutation(train_size)\n",
        "  x_train = x_train[a,...]\n",
        "  y_train = y_train[a]\n",
        "  # Add a new axis\n",
        "  x_train = x_train[..., np.newaxis]\n",
        "  x_test = x_test[..., np.newaxis]\n",
        "\n",
        "\n",
        "  # Convert class vectors to binary class matrices.\n",
        "  y_train = to_categorical(y_train, num_classes)\n",
        "  y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "  # Data normalization\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5_Bu_zpzEeP"
      },
      "source": [
        "Load Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqtta5IZyE-Z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "x_train, y_train, x_test, y_test = load_data('mnist', train_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I9Xh5YHGy_q",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Training samples:\", x_train.shape[0])\n",
        "print(\"Test samples:\", x_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdCMIElKciEM"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idKC7T4qmHPT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "import warnings\n",
        "\n",
        "class EarlyStoppingByLossVal(Callback):\n",
        "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
        "        super(Callback, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.value = value\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
        "\n",
        "        if current < self.value:\n",
        "            if self.verbose > 0:\n",
        "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
        "            self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NosJNCQebjFv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, Dense, MaxPool2D, Dropout, Flatten\n",
        "\n",
        "def get_lenet(l2_reg = 0.0):\n",
        "    tf.keras.utils.set_random_seed(123)\n",
        "\n",
        "    LeNet_l2 = Sequential()\n",
        "    LeNet_l2.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1),\n",
        "                kernel_regularizer=tf.keras.regularizers.L2(l2_reg),\n",
        "                bias_regularizer=tf.keras.regularizers.L2(l2_reg)))\n",
        "    LeNet_l2.add(MaxPool2D(strides=2))\n",
        "    LeNet_l2.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu',\n",
        "                kernel_regularizer=tf.keras.regularizers.L2(l2_reg),\n",
        "                bias_regularizer=tf.keras.regularizers.L2(l2_reg)))\n",
        "    LeNet_l2.add(MaxPool2D(strides=2))\n",
        "    LeNet_l2.add(Flatten())\n",
        "    LeNet_l2.add(Dense(256, activation='relu',\n",
        "                kernel_regularizer=tf.keras.regularizers.L2(l2_reg),\n",
        "                bias_regularizer=tf.keras.regularizers.L2(l2_reg)))\n",
        "    LeNet_l2.add(Dense(84, activation='relu',\n",
        "                kernel_regularizer=tf.keras.regularizers.L2(l2_reg),\n",
        "                bias_regularizer=tf.keras.regularizers.L2(l2_reg)))\n",
        "    LeNet_l2.add(Dense(10))\n",
        "\n",
        "    LeNet_l2.build()\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True\n",
        "    )\n",
        "\n",
        "\n",
        "    adam = Adam(learning_rate=1e-3)\n",
        "    LeNet_l2.compile(loss=cce, metrics=[cce, 'accuracy'], optimizer=adam)\n",
        "    return LeNet_l2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwGNoPYQQdNE"
      },
      "source": [
        "# Training Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Suu7_-Fxt86O",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def model_norm(model):\n",
        "    norm = []\n",
        "    for w in model.get_weights():\n",
        "        norm.append(tf.norm(w))\n",
        "    return np.sum(norm)\n",
        "\n",
        "def model_relative_norm(model, model_reference):\n",
        "    norm = []\n",
        "    for i in range(len(model.get_weights())):\n",
        "        norm.append(tf.norm(model.get_weights()[i]-model_reference.get_weights()[i]))\n",
        "    return np.sum(norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAF9sxkfg8Dv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, epochs, batch_size=train_size, value_stop = 0.05):\n",
        "    callbacks = [EarlyStoppingByLossVal(monitor='categorical_crossentropy', value=value_stop, verbose=1)]\n",
        "    model.fit(x_train, y_train, epochs = epochs, batch_size = batch_size, callbacks = callbacks, verbose = 0)\n",
        "    train_metrics = model.evaluate(x_train, y_train)\n",
        "    test_metrics = model.evaluate(x_test, y_test)\n",
        "    return train_metrics, test_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EijCIDEdhRkO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def eval_jensen(model, lambdas):\n",
        "    cce_red = tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
        "    )\n",
        "    y_pred = model.predict(x_test)\n",
        "    log_p = -cce_red(y_test, y_pred)\n",
        "    return np.array([(tfp.math.reduce_logmeanexp(lamb * log_p) - tf.reduce_mean(lamb * log_p)) for lamb in lambdas])\n",
        "\n",
        "def jensen_function(model, lamb):\n",
        "    cce_red = tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
        "    )\n",
        "    y_pred = model.predict(x_test)\n",
        "    log_p = -cce_red(y_test, y_pred)\n",
        "    return (tfp.math.reduce_logmeanexp(lamb * log_p))/lamb, tf.reduce_mean(log_p), (tfp.math.reduce_logmeanexp(lamb * log_p) - tf.reduce_mean(lamb * log_p))/lamb\n",
        "\n",
        "\n",
        "def model_weights(model):\n",
        "    norm = []\n",
        "    for w in model.get_weights():\n",
        "        norm.append(tf.reshape(w,[-1]))\n",
        "\n",
        "    w_vals = []\n",
        "    for w in norm:\n",
        "      w_vals.append(tf.reshape(w,[-1]))\n",
        "\n",
        "    return tf.concat(w_vals,axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40m6Zz2tilei",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "l2_values = [0.01, 0.0]\n",
        "bach_sizes = [50, train_size]\n",
        "value_stops = [0.01, 0.01]\n",
        "\n",
        "labels = ['SGD-0.3', 'SGD-0.05']\n",
        "#l2_values = [0.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtHOok7g1guT"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIEGu98ih32p",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "models = [get_lenet(l2) for l2 in l2_values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQZ26Mj1iBOH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "metrics = [train_and_evaluate(models[i], 200, batch_size=bach_sizes[i], value_stop = value_stops[i]) for i in range(len(models))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO9wNXYb1kwU"
      },
      "source": [
        "# Plot Jensen-Gap and Rate Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-ODU0FUt2hh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "norms = [model_norm(model) for model in models]\n",
        "norms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICpmAk50iQPZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "lambdas = np.linspace(-0.5, 0.5, 100)\n",
        "jensens = [eval_jensen(model, lambdas) for model in models]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CGajG9PdQGb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGwPVA-cdYM0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "for i in np.arange(len(l2_values)):\n",
        "    plt.plot(lambdas, jensens[i], label = \"{}\".format(labels[i]))\n",
        "    plt.ylim(0,0.1)\n",
        "    plt.xlim(0,0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5TgxZ87Jvar"
      },
      "source": [
        "# Sampling $\\alpha$ for a given model using data sets of size $batch\\_size$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqkNkQARfHEN",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#We fix the i-th\n",
        "batch_vals_all = []\n",
        "\n",
        "for i in np.arange(len(l2_values)):\n",
        "  batch_size=50\n",
        "\n",
        "\n",
        "  batch_vals = []\n",
        "\n",
        "  #L(\\theta)\n",
        "  eval_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "  eval_dataset = eval_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "  metric_test = models[i].evaluate(x_test, y_test, batch_size=batch_size)\n",
        "  L=metric_test[1]\n",
        "\n",
        "  for step, (x_batch, y_batch) in enumerate(eval_dataset):\n",
        "    #\\hatL(D_i,\\theta)\n",
        "    metric_batch = models[i].evaluate(x_batch, y_batch, batch_size=batch_size, verbose=False)\n",
        "    batch_vals.append(metric_batch[1])\n",
        "\n",
        "  batch_vals = np.array(batch_vals)\n",
        "\n",
        "  batch_vals_all.append(batch_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63Jxualsmo95",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "plt.rcParams['figure.figsize'] = (16, 9)\n",
        "fontsize = 30\n",
        "\n",
        "for i in np.arange(len(l2_values)):\n",
        "  batch_vals = batch_vals_all[i]\n",
        "  print(np.mean(batch_vals))\n",
        "  sns.histplot(batch_vals, stat=\"density\", binwidth=0.05, label = r\"Empirical Density of $\\hat{L}(D,\\theta)$\")\n",
        "  plt.axvline(np.mean(batch_vals), 0, 1.0, color=\"red\", label=r\"$L(\\theta)$\", linewidth = 3)\n",
        "  plt.xlim(0,1)\n",
        "  plt.ylim(0,3.5)\n",
        "  plt.legend(prop={'size': 26})\n",
        "  plt.xticks(fontsize=26)\n",
        "  plt.yticks(fontsize=26)\n",
        "  plt.ylabel(\"\", fontsize=26)\n",
        "  plt.savefig(f\"Lhat_density_{i}.pdf\",  format = \"pdf\",bbox_inches='tight')\n",
        "  plt.show()\n",
        "  batch_vals.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "plt.rcParams['figure.figsize'] = (16, 9)\n",
        "fontsize = 30\n",
        "\n",
        "for i in np.arange(len(l2_values)):\n",
        "  batch_vals = batch_vals_all[i]\n",
        "  print(np.mean(batch_vals))\n",
        "  err=np.mean(batch_vals)-batch_vals\n",
        "  sns.histplot(err[err>0], stat=\"density\", binwidth = 0.01, label = r\"Empirical Density of $\\hat{L}(D,\\theta)$\")\n",
        "  #plt.axvline(np.mean(batch_vals), 0, 1.0, color=\"red\", label=r\"$L(\\theta)$\", linewidth = 3)\n",
        "  plt.xlim(0,0.5)\n",
        "  plt.legend(prop={'size': 26})\n",
        "  plt.xticks(fontsize=26)\n",
        "  plt.yticks(fontsize=26)\n",
        "  plt.ylabel(\"\")\n",
        "  plt.savefig(f\"L_Lhat_density_m{i}.pdf\",  format = \"pdf\",bbox_inches='tight')\n",
        "  plt.show()\n",
        "  batch_vals.shape"
      ],
      "metadata": {
        "id": "AI0HZDn2iYI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myJM306XuUuv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "plt.rcParams['figure.figsize'] = (16, 9)\n",
        "fontsize = 30\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "lambda_rate = 1#/np.mean(alpha_vals) #batch_size\n",
        "\n",
        "for i in np.arange(len(l2_values)):\n",
        "  batch_vals = batch_vals_all[i]\n",
        "  L=np.mean(batch_vals)\n",
        "\n",
        "  total_alpha_vals = rate_function(models[i],lambdas,L-batch_vals)\n",
        "  total_alpha_vals = batch_size*np.array(total_alpha_vals)*np.sign(L-batch_vals)\n",
        "\n",
        "  alpha_vals = total_alpha_vals[L>batch_vals]\n",
        "\n",
        "  data = alpha_vals\n",
        "  sns.histplot(alpha_vals, stat=\"density\", bins = 30, label = r\"Empirical Density of $\\alpha(\\theta,D)$\")\n",
        "  x = np.linspace(0, 5, 1000)\n",
        "  y = lambda_rate *np.exp(-lambda_rate * x)\n",
        "  plt.plot(x, y, color='red', label = \"Exp(1) Density\", linewidth = 3)\n",
        "  plt.legend(prop={'size': 26})\n",
        "  plt.xticks(fontsize=26)\n",
        "  plt.yticks(fontsize=26)\n",
        "  plt.ylim(0,2)\n",
        "  plt.xlim(0,5)\n",
        "  plt.ylabel(\"\", fontsize=26)\n",
        "  plt.savefig(f\"alpha_density_m{i}.pdf\",  format = \"pdf\",bbox_inches='tight')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXgPWRt2xNiO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in np.arange(len(l2_values)):\n",
        "  batch_vals = batch_vals_all[i]\n",
        "  L=np.mean(batch_vals)\n",
        "\n",
        "  total_alpha_vals = rate_function(models[i],lambdas,L-batch_vals)\n",
        "  total_alpha_vals = batch_size*np.array(total_alpha_vals)*np.sign(L-batch_vals)\n",
        "\n",
        "  alpha_vals = total_alpha_vals[L>batch_vals]\n",
        "\n",
        "\n",
        "  sns.ecdfplot(alpha_vals, label = r\"Empirical CDF of $\\alpha(\\theta,D)$\", linewidth = 3)\n",
        "  x = np.linspace(0, 5, 1000)\n",
        "  y = 1-np.exp(-lambda_rate * x)\n",
        "  plt.plot(x, y, color='red', label = \"Exp(1) CDF\", linewidth = 3)\n",
        "  plt.legend(prop={'size': 26})\n",
        "  plt.xticks(fontsize=26)\n",
        "  plt.yticks(fontsize=26)\n",
        "  plt.xlim(0,5)\n",
        "  plt.ylabel(\"\")\n",
        "  plt.savefig(f\"alpha_cdf_m{i}.pdf\",  format = \"pdf\",bbox_inches='tight')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfdKKBnfucCC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from scipy.stats import skew, kurtosis\n",
        "print(skew(alpha_vals))\n",
        "print(kurtosis(alpha_vals))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URh9dTx6k6jW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import expon, kstest, anderson\n",
        "\n",
        "# Define rate parameter\n",
        "rate = 1#/np.mean(alpha_vals)\n",
        "print(1/np.mean(alpha_vals))\n",
        "\n",
        "\n",
        "# Perform Kolmogorov-Smirnov test\n",
        "ks_statistic, p_value = kstest(alpha_vals, expon.cdf, args=(0, 1/rate))\n",
        "print('KS test statistic:', ks_statistic)\n",
        "print('KS test p-value:', p_value)\n",
        "\n",
        "# Perform Anderson-Darling test\n",
        "ad_statistic, critical_values, significance_levels = anderson(alpha_vals, 'expon')\n",
        "print('AD test statistic:', ad_statistic)\n",
        "print('AD critical values:', critical_values)\n",
        "print('AD significance levels:', significance_levels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoAnOJ7WzUSC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!zip images.zip *.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('images.zip')\n"
      ],
      "metadata": {
        "id": "m8l9v1ItXLsH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}